{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8+YoWot+grHYrG2FXuFim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siri800/NLP/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnJk3XGrysf",
        "outputId": "a1119cef-2025-4655-aff7-820b13bf3f38"
      },
      "source": [
        "# ----------------------------------------------\n",
        "# AI TEXT ANALYZER â€“ OFFLINE NLP PROJECT (ONE FILE)\n",
        "# Sentiment + Emotion + Keywords + Summarization\n",
        "# ----------------------------------------------\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Download required datasets\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"movie_reviews\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True) # Added to resolve LookupError\n",
        "\n",
        "# -------------------------------\n",
        "# SENTIMENT ANALYZER (OFFLINE)\n",
        "# -------------------------------\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.model = self.train()\n",
        "\n",
        "    def word_features(self, words):\n",
        "        return {w: True for w in words}\n",
        "\n",
        "    def train(self):\n",
        "        pos = [(self.word_features(movie_reviews.words(fid)), \"Positive\")\n",
        "               for fid in movie_reviews.fileids(\"pos\")]\n",
        "        neg = [(self.word_features(movie_reviews.words(fid)), \"Negative\")\n",
        "               for fid in movie_reviews.fileids(\"neg\")]\n",
        "\n",
        "        data = pos + neg\n",
        "        return nltk.NaiveBayesClassifier.train(data)\n",
        "\n",
        "    def predict(self, text):\n",
        "        feats = self.word_features(text.lower().split())\n",
        "        return self.model.classify(feats)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# EMOTION DETECTOR (RULE-BASED)\n",
        "# -------------------------------\n",
        "\n",
        "emotion_words = {\n",
        "    \"Happy\": [\"happy\", \"joy\", \"excited\", \"fun\", \"great\", \"wonderful\"],\n",
        "    \"Sad\": [\"sad\", \"cry\", \"depressed\", \"unhappy\"],\n",
        "    \"Angry\": [\"angry\", \"mad\", \"furious\", \"irritated\"],\n",
        "    \"Fear\": [\"scared\", \"afraid\", \"terrified\", \"fear\"],\n",
        "    \"Love\": [\"love\", \"care\", \"affection\"],\n",
        "}\n",
        "\n",
        "class EmotionDetector:\n",
        "    def predict(self, text):\n",
        "        text = text.lower()\n",
        "        for emo, words in emotion_words.items():\n",
        "            for w in words:\n",
        "                if re.search(rf\"\\b{w}\\b\", text):\n",
        "                    return emo\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# KEYWORD EXTRACTOR (TF-IDF)\n",
        "# -------------------------------\n",
        "\n",
        "class KeywordExtractor:\n",
        "    def extract(self, text, n=5):\n",
        "        tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "        matrix = tfidf.fit_transform([text])\n",
        "        scores = matrix.toarray()[0]\n",
        "        words = tfidf.get_feature_names_out()\n",
        "\n",
        "        top_indices = scores.argsort()[-n:][::-1]\n",
        "        return [words[i] for i in top_indices]\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# TEXT SUMMARIZER (TEXTRANK)\n",
        "# -------------------------------\n",
        "\n",
        "class Summarizer:\n",
        "    def summarize(self, text, n=2):\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        if len(sentences) <= n:\n",
        "            return sentences\n",
        "\n",
        "        tfidf = TfidfVectorizer()\n",
        "        matrix = tfidf.fit_transform(sentences).toarray()\n",
        "        scores = np.sum(matrix, axis=1)\n",
        "        top = scores.argsort()[-n:]\n",
        "        return [sentences[i] for i in top]\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# MAIN APP\n",
        "# -------------------------------\n",
        "\n",
        "def main():\n",
        "    print(\"\\nðŸ”¹ OFFLINE AI TEXT ANALYZER ðŸ”¹\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "    user_text = input(\"\\nEnter your text:\\n> \")\n",
        "\n",
        "    sentiment = SentimentAnalyzer().predict(user_text)\n",
        "    emotion = EmotionDetector().predict(user_text)\n",
        "    keywords = KeywordExtractor().extract(user_text)\n",
        "    summary = Summarizer().summarize(user_text)\n",
        "\n",
        "    print(\"\\nðŸ“Œ Sentiment:\", sentiment)\n",
        "    print(\"ðŸ“Œ Emotion:\", emotion)\n",
        "    print(\"\\nðŸ“Œ Keywords:\")\n",
        "    for k in keywords:\n",
        "        print(\" â€¢\", k)\n",
        "\n",
        "    print(\"\\nðŸ“Œ Summary:\")\n",
        "    for line in summary:\n",
        "        print(\" â€¢\", line)\n",
        "\n",
        "\n",
        "# Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ OFFLINE AI TEXT ANALYZER ðŸ”¹\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter your text:\n",
            "> happy\n",
            "\n",
            "ðŸ“Œ Sentiment: Positive\n",
            "ðŸ“Œ Emotion: Happy\n",
            "\n",
            "ðŸ“Œ Keywords:\n",
            " â€¢ happy\n",
            "\n",
            "ðŸ“Œ Summary:\n",
            " â€¢ happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following commands are shell commands and should be prefixed with '!' in Colab.\n",
        "# Note: Creating a virtual environment like this is generally not necessary in Colab,\n",
        "# as Colab provides its own managed environment. If you want to install packages,\n",
        "# use '!pip install package_name'.\n",
        "\n",
        "# !python -m venv venv\n",
        "# !source venv/bin/activate      # Windows: venv\\Scripts\\activate\n",
        "!pip install nltk scikit-learn numpy matplotlib wordcloud networkx\n",
        "# Optional (recommended for better features):\n",
        "!pip install textstat pyspellchecker langdetect pillow pytesseract spacy gensim\n",
        "# If you want spaCy NER:\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Z0wo_usp7X",
        "outputId": "dae31c94-964c-439c-9975-2ae33f816842"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.11-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.8)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2024.11.6)\n",
            "Downloading textstat-0.7.11-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=89a15b96536baab7f64992044771c7d99e77ec4ec8e2656254502c92a522fd47\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pytesseract, pyspellchecker, pyphen, langdetect, textstat, gensim\n",
            "Successfully installed gensim-4.4.0 langdetect-1.0.9 pyphen-0.17.2 pyspellchecker-0.8.3 pytesseract-0.3.13 textstat-0.7.11\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python super_nlp_tool.py\n",
        "# or analyze a specific text\n",
        "!python super_nlp_tool.py --text \"Your long paragraph here...\"\n",
        "# or analyze the uploaded image (OCR)\n",
        "!python super_nlp_tool.py --image /mnt/data/e23fe506-c61b-49ca-93fb-2c11d9b10a51.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptJQJcmksqpp",
        "outputId": "f5e17997-8ee3-49dd-c32a-104115d1a414"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/super_nlp_tool.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/super_nlp_tool.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/super_nlp_tool.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SUPER NLP TOOL (single-file)\n",
        "Features (offline-first, with optional richer libs):\n",
        "1. Sentiment (NLTK NaiveBayes trained on movie_reviews)\n",
        "2. Emotion detection (rule-based + tiny ML fallback)\n",
        "3. Sarcasm detection (heuristics)\n",
        "4. Toxicity detection (keyword-based)\n",
        "5. Summarization (TextRank-like using TF-IDF + PageRank)\n",
        "6. Keyword extraction (TF-IDF)\n",
        "7. Readability scores (textstat fallback heuristics)\n",
        "8. Grammar/spelling correction (pyspellchecker fallback)\n",
        "9. NER (spaCy if available, else regex heuristics)\n",
        "10. Language detection (langdetect if available, else heuristic)\n",
        "11. Text statistics (counts, lexical richness)\n",
        "12. WordCloud generation (saves PNG)\n",
        "13. Topic modeling (LDA via gensim, optional)\n",
        "14. Plagiarism similarity (cosine TF-IDF between two texts)\n",
        "15. Uses OCR on uploaded image if pytesseract/PIL available\n",
        "\n",
        "Uploaded file path from environment (if you want to test OCR):\n",
        "/mnt/data/e23fe506-c61b-49ca-93fb-2c11d9b10a51.jpg\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import math\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "\n",
        "# Core libs\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import networkx as nx\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Try optional libraries (graceful)\n",
        "try:\n",
        "    import textstat\n",
        "except Exception:\n",
        "    textstat = None\n",
        "\n",
        "try:\n",
        "    from spellchecker import SpellChecker\n",
        "except Exception:\n",
        "    SpellChecker = None\n",
        "\n",
        "try:\n",
        "    from langdetect import detect as lang_detect\n",
        "except Exception:\n",
        "    lang_detect = None\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception:\n",
        "        nlp_spacy = None\n",
        "except Exception:\n",
        "    spacy = None\n",
        "    nlp_spacy = None\n",
        "\n",
        "try:\n",
        "    from PIL import Image\n",
        "    import pytesseract\n",
        "except Exception:\n",
        "    Image = None\n",
        "    pytesseract = None\n",
        "\n",
        "try:\n",
        "    import gensim\n",
        "    from gensim import corpora\n",
        "    from gensim.models.ldamodel import LdaModel\n",
        "except Exception:\n",
        "    gensim = None\n",
        "\n",
        "# Ensure required NLTK data\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"movie_reviews\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# Uploaded file path (from conversation history / environment)\n",
        "UPLOADED_IMAGE_PATH = \"/mnt/data/e23fe506-c61b-49ca-93fb-2c11d9b10a51.jpg\"\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 1) SENTIMENT (NLTK NB)\n",
        "# ------------------------\n",
        "class SentimentModel:\n",
        "    def __init__(self):\n",
        "        # Train NLTK NaiveBayes on movie_reviews (fast enough)\n",
        "        try:\n",
        "            pos_ids = movie_reviews.fileids(\"pos\")\n",
        "            neg_ids = movie_reviews.fileids(\"neg\")\n",
        "            data = []\n",
        "            for fid in pos_ids:\n",
        "                words = movie_reviews.words(fid)\n",
        "                data.append((self.featureize(words), \"Positive\"))\n",
        "            for fid in neg_ids:\n",
        "                words = movie_reviews.words(fid)\n",
        "                data.append((self.featureize(words), \"Negative\"))\n",
        "            # Shuffle small dataset\n",
        "            import random\n",
        "            random.shuffle(data)\n",
        "            self.classifier = nltk.NaiveBayesClassifier.train(data)\n",
        "        except Exception as e:\n",
        "            print(\"Warning: could not train NLTK NB (movie_reviews). Error:\", e)\n",
        "            self.classifier = None\n",
        "\n",
        "    def featureize(self, words):\n",
        "        return {w.lower(): True for w in words if len(w) > 2}\n",
        "\n",
        "    def predict(self, text):\n",
        "        if not self.classifier:\n",
        "            # fallback: simple polarity by counting good/bad words\n",
        "            pos_words = {\"good\", \"great\", \"excellent\", \"love\", \"wonderful\", \"best\", \"fantastic\", \"amazing\", \"enjoy\"}\n",
        "            neg_words = {\"bad\", \"terrible\", \"worst\", \"hate\", \"awful\", \"boring\", \"disappoint\"}\n",
        "            toks = [t.lower() for t in word_tokenize(text)]\n",
        "            s = sum(1 for t in toks if t in pos_words) - sum(1 for t in toks if t in neg_words)\n",
        "            label = \"Positive\" if s >= 0 else \"Negative\"\n",
        "            score = abs(s) / max(1, len(toks))\n",
        "            return {\"label\": label, \"score\": float(score)}\n",
        "        feats = self.featureize(word_tokenize(text))\n",
        "        prob_dist = self.classifier.prob_classify(feats)\n",
        "        label = prob_dist.max()\n",
        "        score = prob_dist.prob(label)\n",
        "        return {\"label\": label, \"score\": float(score)}\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 2) EMOTION (rule-based)\n",
        "# ------------------------\n",
        "EMOTION_KEYWORDS = {\n",
        "    \"Joy\": [\"happy\", \"delighted\", \"excited\", \"glad\", \"wonderful\", \"pleased\"],\n",
        "    \"Sadness\": [\"sad\", \"depressed\", \"unhappy\", \"sorrow\", \"cry\", \"mourn\"],\n",
        "    \"Anger\": [\"angry\", \"mad\", \"furious\", \"irritated\", \"annoyed\"],\n",
        "    \"Fear\": [\"scared\", \"fear\", \"afraid\", \"terrified\", \"anxious\"],\n",
        "    \"Surprise\": [\"surprised\", \"astonished\", \"amazed\", \"shocked\"],\n",
        "    \"Love\": [\"love\", \"adore\", \"cherish\", \"fond\"],\n",
        "}\n",
        "\n",
        "def detect_emotion(text):\n",
        "    t = text.lower()\n",
        "    counts = {}\n",
        "    for emo, words in EMOTION_KEYWORDS.items():\n",
        "        counts[emo] = sum(1 for w in words if re.search(rf\"\\b{re.escape(w)}\\b\", t))\n",
        "    # Choose highest count, if tie choose neutral\n",
        "    best = max(counts.items(), key=lambda x: x[1])\n",
        "    if best[1] == 0:\n",
        "        return \"Neutral\"\n",
        "    return best[0]\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 3) SARCASM (heuristics)\n",
        "# ------------------------\n",
        "SARCASM_PATTERNS = [\n",
        "    r\"\\byeah, right\\b\",\n",
        "    r\"\\bsure, (?:like )?that\\b\",\n",
        "    r\"\\bI (?:just )?love when\\b\",\n",
        "    r\"\\b(as if)\\b\",\n",
        "    r\"\\bgreat\\.\\b\",  # 'Great.' often sarcastic with a period\n",
        "]\n",
        "\n",
        "def detect_sarcasm(text):\n",
        "    t = text.lower()\n",
        "    # exclamation + negative emoticons? heuristic\n",
        "    if re.search(r\":\\)|:\\(|;-\\)\", text):\n",
        "        # emoticons alone are not sarcasm; skip\n",
        "        pass\n",
        "    for pat in SARCASM_PATTERNS:\n",
        "        if re.search(pat, t):\n",
        "            return True\n",
        "    # punctuation heuristics: excessive quotes or heavy punctuation combined with negative words\n",
        "    if '\"' in text and any(w in t for w in [\"great\", \"love\", \"amazing\"]):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 4) TOXICITY (keywords)\n",
        "# ------------------------\n",
        "TOXIC_KEYWORDS = set([\n",
        "    \"stupid\", \"idiot\", \"dumb\", \"hate\", \"kill\", \"trash\", \"sucks\", \"trash\", \"jerk\", \"f***\", \"b****\", \"shut up\"\n",
        "])\n",
        "def detect_toxicity(text):\n",
        "    toks = [w.lower() for w in word_tokenize(text)]\n",
        "    hits = [w for w in toks if w in TOXIC_KEYWORDS]\n",
        "    score = len(hits) / max(1, len(toks))\n",
        "    label = \"Toxic\" if hits else \"Clean\"\n",
        "    return {\"label\": label, \"score\": float(score), \"hits\": list(set(hits))}\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 5) SUMMARIZER (TextRank-like)\n",
        "# ------------------------\n",
        "def summarize_text(text, num_sentences=2):\n",
        "    sents = sent_tokenize(text)\n",
        "    if len(sents) <= num_sentences:\n",
        "        return sents\n",
        "    # TF-IDF vectors of sentences\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(sents)\n",
        "    sim_matrix = (X * X.T).toarray()\n",
        "    # zero diagonal\n",
        "    np.fill_diagonal(sim_matrix, 0)\n",
        "    # PageRank\n",
        "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "    try:\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "    except Exception:\n",
        "        # fallback: sum of similarities\n",
        "        scores = {i: sim_matrix[i].sum() for i in range(len(sents))}\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    selected_idx = [i for i, _ in ranked[:num_sentences]]\n",
        "    selected_idx.sort()\n",
        "    return [sents[i] for i in selected_idx]\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 6) KEYWORDS (TF-IDF)\n",
        "# ------------------------\n",
        "def extract_keywords(text, top_k=8):\n",
        "    v = TfidfVectorizer(stop_words=\"english\", max_features=2000)\n",
        "    X = v.fit_transform([text])\n",
        "    scores = X.toarray()[0]\n",
        "    if not scores.any():\n",
        "        return []\n",
        "    words = v.get_feature_names_out()\n",
        "    indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [words[i] for i in indices if scores[i] > 0.0]\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 7) READABILITY\n",
        "# ------------------------\n",
        "def readability_scores(text):\n",
        "    if textstat:\n",
        "        flesch = textstat.flesch_reading_ease(text)\n",
        "        grade = textstat.text_standard(text, float_output=True)\n",
        "        return {\"flesch_reading_ease\": flesch, \"grade_level\": grade}\n",
        "    # fallback simple heuristic: avg words per sentence\n",
        "    sents = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    if not sents:\n",
        "        return {\"flesch_reading_ease\": None, \"grade_level\": None}\n",
        "    avg_words = len(words) / len(sents)\n",
        "    # map avg_words to rough grade (very rough)\n",
        "    if avg_words < 12:\n",
        "        grade = 6\n",
        "    elif avg_words < 18:\n",
        "        grade = 9\n",
        "    else:\n",
        "        grade = 12\n",
        "    flesch = 206.835 - 1.015 * avg_words - 84.6 * (sum(len(w) for w in words) / max(1, len(words)))/len(words)\n",
        "    return {\"flesch_reading_ease\": round(flesch,2), \"grade_level\": grade}\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 8) GRAMMAR & SPELLING (pyspellchecker fallback)\n",
        "# ------------------------\n",
        "def spelling_corrections(text, max_suggestions=3):\n",
        "    if SpellChecker:\n",
        "        sp = SpellChecker()\n",
        "        tokens = [t for t in word_tokenize(text) if t.isalpha()]\n",
        "        miss = sp.unknown(tokens)\n",
        "        suggestions = {w: list(sp.candidates(w))[:max_suggestions] for w in miss}\n",
        "        return suggestions\n",
        "    else:\n",
        "        return {}  # no spellchecker available\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 9) NER (spaCy or regex heuristics)\n",
        "# ------------------------\n",
        "def extract_entities(text):\n",
        "    if nlp_spacy:\n",
        "        doc = nlp_spacy(text)\n",
        "        return [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n",
        "    # fallback heuristics: dates, caps words, simple patterns\n",
        "    ents = []\n",
        "    # Dates (very simple)\n",
        "    dates = re.findall(r\"\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b \\d{1,2},? \\d{4})\", text, flags=re.IGNORECASE)\n",
        "    for d in dates:\n",
        "        ents.append({\"text\": d, \"label\": \"DATE\"})\n",
        "    # Proper noun sequences\n",
        "    candidates = re.findall(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b\", text)\n",
        "    for c in candidates:\n",
        "        ents.append({\"text\": c, \"label\": \"PERSON_OR_ORG\"})\n",
        "    return ents\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 10) LANGUAGE DETECTION\n",
        "# ------------------------\n",
        "def detect_language(text):\n",
        "    if lang_detect:\n",
        "        try:\n",
        "            return lang_detect(text)\n",
        "        except Exception:\n",
        "            return \"unknown\"\n",
        "    # fallback: simple detection based on common character sets\n",
        "    if re.search(r\"[à¤…à¤†à¤‡à¤ˆà¤‰à¤Šà¤à¤à¤“à¤”]\", text):\n",
        "        return \"hi\"  # Indic script heuristic\n",
        "    if re.search(r\"[Ð°-ÑÐ-Ð¯]\", text):\n",
        "        return \"ru\"\n",
        "    return \"en\"\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 11) TEXT STATS\n",
        "# ------------------------\n",
        "def text_statistics(text):\n",
        "    words = [w for w in word_tokenize(text) if any(c.isalpha() for c in w)]\n",
        "    sents = sent_tokenize(text)\n",
        "    chars = len(text)\n",
        "    unique = len(set(w.lower() for w in words))\n",
        "    lexical_richness = unique / max(1, len(words))\n",
        "    avg_word_len = sum(len(w) for w in words) / max(1, len(words))\n",
        "    return {\n",
        "        \"words\": len(words),\n",
        "        \"sentences\": len(sents),\n",
        "        \"characters\": chars,\n",
        "        \"unique_words\": unique,\n",
        "        \"lexical_richness\": round(lexical_richness, 3),\n",
        "        \"avg_word_length\": round(avg_word_len, 2)\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 12) WORDCLOUD (saves figure)\n",
        "# ------------------------\n",
        "def save_wordcloud(text, out_path=\"wordcloud.png\"):\n",
        "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 13) TOPIC MODELING (LDA optional)\n",
        "# ------------------------\n",
        "def topic_modeling(texts, num_topics=3, num_words=6):\n",
        "    if not gensim:\n",
        "        return {\"warning\": \"gensim not installed, skipping LDA\"}\n",
        "    # prepare\n",
        "    tokenized = [[w.lower() for w in word_tokenize(re.sub(r\"[^a-zA-Z0-9\\\\s]\", \" \", t)) if len(w)>2] for t in texts]\n",
        "    dictionary = corpora.Dictionary(tokenized)\n",
        "    corpus = [dictionary.doc2bow(t) for t in tokenized]\n",
        "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=5)\n",
        "    topics = {}\n",
        "    for i in range(num_topics):\n",
        "        topics[f\"topic_{i}\"] = [w for w, _ in lda.show_topic(i, topn=num_words)]\n",
        "    return topics\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 14) PLAGIARISM (cosine TF-IDF)\n",
        "# ------------------------\n",
        "def similarity_score(text1, text2):\n",
        "    v = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = v.fit_transform([text1, text2])\n",
        "    sim = float(cosine_similarity(X[0], X[1])[0][0])\n",
        "    return sim\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 15) OCR helper (optional)\n",
        "# ------------------------\n",
        "def ocr_image(image_path):\n",
        "    if pytesseract and Image:\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            text = pytesseract.image_to_string(img)\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            return f\"OCR error: {e}\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# CLI / Run orchestration\n",
        "# ------------------------\n",
        "def analyze_text_full(text, extra_compare_text=None, save_wordcloud_flag=False):\n",
        "    result = {}\n",
        "    # Sentiment\n",
        "    sent = SentimentModel().predict(text)\n",
        "    result[\"sentiment\"] = sent\n",
        "    # Emotion\n",
        "    result[\"emotion\"] = detect_emotion(text)\n",
        "    # Sarcasm\n",
        "    result[\"sarcasm\"] = detect_sarcasm(text)\n",
        "    # Toxicity\n",
        "    result[\"toxicity\"] = detect_toxicity(text)\n",
        "    # Summary\n",
        "    result[\"summary\"] = summarize_text(text, num_sentences=3)\n",
        "    # Keywords\n",
        "    result[\"keywords\"] = extract_keywords(text, top_k=10)\n",
        "    # Readability\n",
        "    result[\"readability\"] = readability_scores(text)\n",
        "    # Spelling corrections\n",
        "    result[\"spelling_suggestions\"] = spelling_corrections(text)\n",
        "    # Entities\n",
        "    result[\"entities\"] = extract_entities(text)\n",
        "    # Language\n",
        "    result[\"language\"] = detect_language(text)\n",
        "    # Stats\n",
        "    result[\"stats\"] = text_statistics(text)\n",
        "    # WordCloud\n",
        "    if save_wordcloud_flag:\n",
        "        wc_path = save_wordcloud(text, out_path=\"wordcloud_output.png\")\n",
        "        result[\"wordcloud_path\"] = wc_path\n",
        "    # Topic modeling: run on sentence-level texts (small)\n",
        "    try:\n",
        "        sents = sent_tokenize(text)\n",
        "        if len(sents) >= 3 and gensim:\n",
        "            result[\"topics\"] = topic_modeling(sents, num_topics=min(3,len(sents)))\n",
        "        else:\n",
        "            result[\"topics\"] = {}\n",
        "    except Exception:\n",
        "        result[\"topics\"] = {}\n",
        "    # similarity if extra text provided\n",
        "    if extra_compare_text:\n",
        "        result[\"similarity_with_other_text\"] = similarity_score(text, extra_compare_text)\n",
        "    return result\n",
        "\n",
        "\n",
        "def pretty_print_result(res):\n",
        "    print(\"\\n===== ANALYSIS SUMMARY =====\\n\")\n",
        "    print(\"Sentiment:\", res[\"sentiment\"])\n",
        "    print(\"Emotion:\", res[\"emotion\"])\n",
        "    print(\"Sarcasm detected:\", res[\"sarcasm\"])\n",
        "    print(\"Toxicity:\", res[\"toxicity\"])\n",
        "    print(\"\\nSummary:\")\n",
        "    for s in res[\"summary\"]:\n",
        "        print(\" â€¢\", s)\n",
        "    print(\"\\nTop Keywords:\", res[\"keywords\"][:10])\n",
        "    print(\"\\nReadability:\", res[\"readability\"])\n",
        "    print(\"\\nSpelling suggestions (if any):\", res[\"spelling_suggestions\"])\n",
        "    print(\"\\nEntities:\", res[\"entities\"])\n",
        "    print(\"\\nLanguage:\", res[\"language\"])\n",
        "    print(\"\\nText stats:\", res[\"stats\"])\n",
        "    if \"topics\" in res:\n",
        "        print(\"\\nTopics:\", res[\"topics\"])\n",
        "    if \"similarity_with_other_text\" in res:\n",
        "        print(\"\\nSimilarity with other text:\", res[\"similarity_with_other_text\"])\n",
        "    if \"wordcloud_path\" in res:\n",
        "        print(\"\\nWordcloud saved to:\", res[\"wordcloud_path\"])\n",
        "    print(\"\\n===========================\\n\")\n",
        "\n",
        "\n",
        "def main_cli():\n",
        "    parser = argparse.ArgumentParser(description=\"Super NLP Tool - offline-first\")\n",
        "    parser.add_argument(\"--text\", type=str, help=\"Text to analyze (wrap in quotes)\")\n",
        "    parser.add_argument(\"--image\", type=str, help=\"Image path to OCR and analyze\")\n",
        "    parser.add_argument(\"--compare\", type=str, help=\"Extra text to compute similarity against\")\n",
        "    parser.add_argument(\"--wordcloud\", action=\"store_true\", help=\"Save wordcloud image\")\n",
        "\n",
        "    # Filter sys.argv to remove Jupyter/IPython specific arguments\n",
        "    filtered_args = []\n",
        "    skip_next = False\n",
        "    for i, arg in enumerate(sys.argv[1:]):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        if arg == '-f': # Found the kernel argument flag\n",
        "            skip_next = True # Skip the next argument (the kernel file path)\n",
        "        else:\n",
        "            filtered_args.append(arg)\n",
        "\n",
        "    args = parser.parse_args(filtered_args) # Pass the filtered arguments\n",
        "\n",
        "    input_text = None\n",
        "    if args.image:\n",
        "        ocr = ocr_image(args.image)\n",
        "        if ocr is None:\n",
        "            print(\"OCR unavailable; please install pytesseract and pillow for OCR.\")\n",
        "            return\n",
        "        print(\"OCR extracted text:\")\n",
        "        print(ocr[:500] + (\"...\" if len(ocr) > 500 else \"\"))\n",
        "        input_text = ocr\n",
        "    elif args.text:\n",
        "        input_text = args.text\n",
        "    else:\n",
        "        print(\"Enter/Paste text (blank line to finish):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            try:\n",
        "                ln = input()\n",
        "            except EOFError:\n",
        "                break\n",
        "            if ln.strip() == \"\":\n",
        "                break\n",
        "            lines.append(ln)\n",
        "        input_text = \"\\n\".join(lines)\n",
        "\n",
        "    if not input_text.strip():\n",
        "        print(\"No text provided. Exiting.\")\n",
        "        return\n",
        "\n",
        "    res = analyze_text_full(input_text, extra_compare_text=args.compare, save_wordcloud_flag=args.wordcloud)\n",
        "    pretty_print_result(res)\n",
        "    # Optionally show wordcloud if created\n",
        "    if args.wordcloud and res.get(\"wordcloud_path\"):\n",
        "        try:\n",
        "            img = Image.open(res[\"wordcloud_path\"])\n",
        "            img.show()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Print helpful header\n",
        "    print(\"Super NLP Tool â€” all features (offline-first).\")\n",
        "    print(\"Uploaded sample image path available (if you want to test OCR):\")\n",
        "    print(\"  \", UPLOADED_IMAGE_PATH)\n",
        "    main_cli()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjjx98vatYDI",
        "outputId": "de6dd6fc-daf9-4e4f-e2ea-a00734bcd4cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Super NLP Tool â€” all features (offline-first).\n",
            "Uploaded sample image path available (if you want to test OCR):\n",
            "   /mnt/data/e23fe506-c61b-49ca-93fb-2c11d9b10a51.jpg\n",
            "Enter/Paste text (blank line to finish):\n",
            "I am very happy today because I got good marks.\n",
            "I am so happy today because everything is going perfectly!\n",
            "\n",
            "\n",
            "===== ANALYSIS SUMMARY =====\n",
            "\n",
            "Sentiment: {'label': 'Positive', 'score': 0.8723532906041606}\n",
            "Emotion: Joy\n",
            "Sarcasm detected: False\n",
            "Toxicity: {'label': 'Clean', 'score': 0.0, 'hits': []}\n",
            "\n",
            "Summary:\n",
            " â€¢ I am very happy today because I got good marks.\n",
            " â€¢ I am so happy today because everything is going perfectly!\n",
            "\n",
            "Top Keywords: ['today', 'happy', 'perfectly', 'marks', 'got', 'good', 'going']\n",
            "\n",
            "Readability: {'flesch_reading_ease': 61.32500000000002, 'grade_level': 6.0}\n",
            "\n",
            "Spelling suggestions (if any): {}\n",
            "\n",
            "Entities: [{'text': 'today', 'label': 'DATE'}, {'text': 'today', 'label': 'DATE'}]\n",
            "\n",
            "Language: en\n",
            "\n",
            "Text stats: {'words': 20, 'sentences': 2, 'characters': 106, 'unique_words': 14, 'lexical_richness': 0.7, 'avg_word_length': 4.25}\n",
            "\n",
            "Topics: {}\n",
            "\n",
            "===========================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}